{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan\n",
    "\n",
    "# Returns the list top 3 elements or entire list; whichever is more.\n",
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []\n",
    "\n",
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credits = pd.read_csv(\"tmdb_5000_credits.csv\", sep = \",\")\n",
    "df_movies = pd.read_csv(\"tmdb_5000_movies.csv\", sep = \",\")\n",
    "df_imdb = pd.read_csv(\"imdb_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_col = ['budget', 'genres', 'id', 'popularity', 'revenue', 'runtime', 'title', 'num_critic_for_reviews', 'duration'\n",
    "            , 'actor_1_facebook_likes', 'director_facebook_likes', 'actor_3_facebook_likes', 'gross', 'cast_total_facebook_likes'\n",
    "            , 'facenumber_in_poster', 'num_user_for_reviews', 'actor_2_facebook_likes', 'imdb_score', 'aspect_ratio', \"movie_facebook_likes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_nn = df_imdb.loc[:, used_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_nn.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df_imdb_nn.loc[:, 'genres'] = df_imdb_nn.loc[:, 'genres'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_nn.loc[:, 'genres'] = df_imdb_nn.loc[:, 'genres'].apply(get_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_nn.loc[:, \"genres\"] = df_imdb_nn.loc[:, 'genres'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_nn.loc[:, 'genres_str'] = df_imdb_nn.loc[:, 'genres'].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(df_imdb_nn['genres_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres = pd.DataFrame(index = df_imdb_nn.index, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres_t = df_genres.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_genres.index:\n",
    "    df_genres_t.loc[:, i] = cv.transform([df_imdb_nn[\"genres_str\"][i]]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_feature = ['budget', 'popularity', 'revenue', 'num_critic_for_reviews', 'duration', 'actor_1_facebook_likes',\n",
    "       'director_facebook_likes', 'actor_3_facebook_likes', 'gross', 'cast_total_facebook_likes', 'facenumber_in_poster',\n",
    "       'num_user_for_reviews', 'actor_2_facebook_likes', 'aspect_ratio', 'movie_facebook_likes']\n",
    "X = df_imdb_nn[nn_feature]\n",
    "X = X.merge(df_genres, left_index = True, right_index = True, how = \"left\")\n",
    "\n",
    "y = df_imdb_nn['imdb_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "count = 1\n",
    "for i in np.arange(0.1, 10.1, step = 0.1):\n",
    "    i = round(i, 1)\n",
    "    score_dict[str(i)] = count\n",
    "    count += 1\n",
    "\n",
    "y = y.apply(lambda x : score_dict[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_std_scale = std_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_std_train, X_std_test, y_std_train, y_std_test = train_test_split(X_std_scale, y, test_size = 0.2, random_state = 42)\n",
    "X_std_train, X_std_val, y_std_train, y_std_val = train_test_split(X_std_train, y_std_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train, 100)\n",
    "y_val = np_utils.to_categorical(y_val, 100)\n",
    "y_test = np_utils.to_categorical(y_test, 100)\n",
    "\n",
    "y_std_train = np_utils.to_categorical(y_std_train, 100)\n",
    "y_std_val = np_utils.to_categorical(y_std_val, 100)\n",
    "y_std_test = np_utils.to_categorical(y_std_test, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Sparse AE pre-training on Standarized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "kl_divergence = keras.losses.kullback_leibler_divergence\n",
    "\n",
    "class KLDivergenceRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, weight, target=0.1):\n",
    "        self.weight = weight\n",
    "        self.target = target\n",
    "    def __call__(self, inputs):\n",
    "        mean_activities = K.mean(inputs, axis=0)\n",
    "        return self.weight * (\n",
    "            kl_divergence(self.target, mean_activities) +\n",
    "            kl_divergence(1. - self.target, 1. - mean_activities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2843 samples, validate on 711 samples\n",
      "Epoch 1/64\n",
      "2843/2843 [==============================] - 1s 401us/sample - loss: 0.7868 - val_loss: 0.5670\n",
      "Epoch 2/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.5386 - val_loss: 0.4875\n",
      "Epoch 3/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4581 - val_loss: 0.4586\n",
      "Epoch 4/64\n",
      "2843/2843 [==============================] - 0s 82us/sample - loss: 0.4265 - val_loss: 0.4446\n",
      "Epoch 5/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4106 - val_loss: 0.4340\n",
      "Epoch 6/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4035 - val_loss: 0.4268\n",
      "Epoch 7/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3973 - val_loss: 0.4246\n",
      "Epoch 8/64\n",
      "2843/2843 [==============================] - 0s 104us/sample - loss: 0.3952 - val_loss: 0.4205\n",
      "Epoch 9/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3933 - val_loss: 0.4233\n",
      "Epoch 10/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3947 - val_loss: 0.4169\n",
      "Epoch 11/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4000 - val_loss: 0.4173\n",
      "Epoch 12/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3952 - val_loss: 0.4268\n",
      "Epoch 13/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4251 - val_loss: 0.4148\n",
      "Epoch 14/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.4222 - val_loss: 0.4121\n",
      "Epoch 15/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4168 - val_loss: 0.4087\n",
      "Epoch 16/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4176 - val_loss: 0.4094\n",
      "Epoch 17/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4136 - val_loss: 0.4070\n",
      "Epoch 18/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4197 - val_loss: 0.4075\n",
      "Epoch 19/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4183 - val_loss: 0.4102\n",
      "Epoch 20/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4231 - val_loss: 0.4081\n",
      "Epoch 21/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4143 - val_loss: 0.4100\n",
      "Epoch 22/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4257 - val_loss: 0.4161\n",
      "Epoch 23/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4381 - val_loss: 0.4248\n",
      "Epoch 24/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4423 - val_loss: 0.4283\n",
      "Epoch 25/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4413 - val_loss: 0.4220\n",
      "Epoch 26/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4402 - val_loss: 0.4230\n",
      "Epoch 27/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4429 - val_loss: 0.4221\n",
      "Epoch 28/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4405 - val_loss: 0.4230\n",
      "Epoch 29/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.4407 - val_loss: 0.4234\n",
      "Epoch 30/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4393 - val_loss: 0.4204\n",
      "Epoch 31/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.4369 - val_loss: 0.4176\n",
      "Epoch 32/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4342 - val_loss: 0.4166\n",
      "Epoch 33/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4353 - val_loss: 0.4184\n",
      "Epoch 34/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4403 - val_loss: 0.4167\n",
      "Epoch 35/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4328 - val_loss: 0.4102\n",
      "Epoch 36/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4377 - val_loss: 0.4263\n",
      "Epoch 37/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4421 - val_loss: 0.4240\n",
      "Epoch 38/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.4409 - val_loss: 0.4247\n",
      "Epoch 39/64\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 0.4364 - val_loss: 0.4163\n",
      "Epoch 40/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4356 - val_loss: 0.4185\n",
      "Epoch 41/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4424 - val_loss: 0.4207\n",
      "Epoch 42/64\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 0.4385 - val_loss: 0.4150\n",
      "Epoch 43/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.4241 - val_loss: 0.4184\n",
      "Epoch 44/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.4149 - val_loss: 0.4027\n",
      "Epoch 45/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3938 - val_loss: 0.3765\n",
      "Epoch 46/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3862 - val_loss: 0.3743\n",
      "Epoch 47/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3823 - val_loss: 0.3730\n",
      "Epoch 48/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3807 - val_loss: 0.3723\n",
      "Epoch 49/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3796 - val_loss: 0.3710\n",
      "Epoch 50/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3825 - val_loss: 0.3709\n",
      "Epoch 51/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3830 - val_loss: 0.3736\n",
      "Epoch 52/64\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 0.3837 - val_loss: 0.3737\n",
      "Epoch 53/64\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 0.3860 - val_loss: 0.3730\n",
      "Epoch 54/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3812 - val_loss: 0.3713\n",
      "Epoch 55/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3793 - val_loss: 0.3697\n",
      "Epoch 56/64\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 0.3782 - val_loss: 0.3689\n",
      "Epoch 57/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.3791 - val_loss: 0.3718\n",
      "Epoch 58/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3787 - val_loss: 0.3696\n",
      "Epoch 59/64\n",
      "2843/2843 [==============================] - 0s 104us/sample - loss: 0.3790 - val_loss: 0.3700\n",
      "Epoch 60/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3806 - val_loss: 0.3697\n",
      "Epoch 61/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.3785 - val_loss: 0.3746\n",
      "Epoch 62/64\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 0.4098 - val_loss: 0.3811\n",
      "Epoch 63/64\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 0.4046 - val_loss: 0.3750\n",
      "Epoch 64/64\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 0.3832 - val_loss: 0.3731\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\n",
    "sparse_l1_encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, input_dim = 35, activation = \"relu\"),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(25, activation=\"relu\", activity_regularizer=kld_reg)  # Alternatively, you could add\n",
    "                                                  # activity_regularizer=keras.regularizers.l1(1e-3)\n",
    "                                                  # to the previous layer.\n",
    "])\n",
    "sparse_l1_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"relu\", input_shape=[25]),\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    keras.layers.Dense(35, activation=\"relu\")\n",
    "])\n",
    "\n",
    "sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
    "sparse_l1_ae.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "history = sparse_l1_ae.fit(X_std_train, X_std_train, batch_size = 32, epochs=64,\n",
    "                           validation_data=[X_std_val, X_std_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2843 samples, validate on 711 samples\n",
      "Epoch 1/32\n",
      "2843/2843 [==============================] - 1s 412us/sample - loss: 4.3156 - accuracy: 0.0369 - val_loss: 3.9570 - val_accuracy: 0.0478\n",
      "Epoch 2/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.8694 - accuracy: 0.0440 - val_loss: 3.8218 - val_accuracy: 0.0549\n",
      "Epoch 3/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.7261 - accuracy: 0.0489 - val_loss: 3.7087 - val_accuracy: 0.0549\n",
      "Epoch 4/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.5998 - accuracy: 0.0658 - val_loss: 3.6149 - val_accuracy: 0.0577\n",
      "Epoch 5/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.5032 - accuracy: 0.0823 - val_loss: 3.5941 - val_accuracy: 0.0506\n",
      "Epoch 6/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.4334 - accuracy: 0.0904 - val_loss: 3.5961 - val_accuracy: 0.0689\n",
      "Epoch 7/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.3857 - accuracy: 0.0950 - val_loss: 3.6148 - val_accuracy: 0.0703\n",
      "Epoch 8/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.3380 - accuracy: 0.1038 - val_loss: 3.6005 - val_accuracy: 0.0759\n",
      "Epoch 9/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.2763 - accuracy: 0.1161 - val_loss: 3.6565 - val_accuracy: 0.0788\n",
      "Epoch 10/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.2459 - accuracy: 0.1199 - val_loss: 3.7195 - val_accuracy: 0.0563\n",
      "Epoch 11/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.2147 - accuracy: 0.1280 - val_loss: 3.6880 - val_accuracy: 0.0717\n",
      "Epoch 12/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.1558 - accuracy: 0.1319 - val_loss: 3.7374 - val_accuracy: 0.0647\n",
      "Epoch 13/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.1156 - accuracy: 0.1463 - val_loss: 3.7485 - val_accuracy: 0.0689\n",
      "Epoch 14/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 3.0549 - accuracy: 0.1572 - val_loss: 3.7410 - val_accuracy: 0.0703\n",
      "Epoch 15/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 3.0127 - accuracy: 0.1685 - val_loss: 3.8377 - val_accuracy: 0.0689\n",
      "Epoch 16/32\n",
      "2843/2843 [==============================] - 0s 88us/sample - loss: 2.9542 - accuracy: 0.1864 - val_loss: 3.8360 - val_accuracy: 0.0689\n",
      "Epoch 17/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.8935 - accuracy: 0.1963 - val_loss: 3.9457 - val_accuracy: 0.0689\n",
      "Epoch 18/32\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 2.8600 - accuracy: 0.1945 - val_loss: 4.0496 - val_accuracy: 0.0619\n",
      "Epoch 19/32\n",
      "2843/2843 [==============================] - 0s 104us/sample - loss: 2.7890 - accuracy: 0.2184 - val_loss: 4.0400 - val_accuracy: 0.0717\n",
      "Epoch 20/32\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 2.7366 - accuracy: 0.2244 - val_loss: 4.0719 - val_accuracy: 0.0661\n",
      "Epoch 21/32\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 2.6668 - accuracy: 0.2547 - val_loss: 4.2833 - val_accuracy: 0.0534\n",
      "Epoch 22/32\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 2.6179 - accuracy: 0.2571 - val_loss: 4.3877 - val_accuracy: 0.0520\n",
      "Epoch 23/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.5637 - accuracy: 0.2733 - val_loss: 4.3899 - val_accuracy: 0.0619\n",
      "Epoch 24/32\n",
      "2843/2843 [==============================] - 0s 99us/sample - loss: 2.5022 - accuracy: 0.2948 - val_loss: 4.6595 - val_accuracy: 0.0577\n",
      "Epoch 25/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.4584 - accuracy: 0.3180 - val_loss: 4.5720 - val_accuracy: 0.0436\n",
      "Epoch 26/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.4023 - accuracy: 0.3292 - val_loss: 4.8716 - val_accuracy: 0.0605\n",
      "Epoch 27/32\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 2.3069 - accuracy: 0.3528 - val_loss: 4.8742 - val_accuracy: 0.0534\n",
      "Epoch 28/32\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 2.2909 - accuracy: 0.3577 - val_loss: 5.2122 - val_accuracy: 0.0450\n",
      "Epoch 29/32\n",
      "2843/2843 [==============================] - 0s 110us/sample - loss: 2.4637 - accuracy: 0.3071 - val_loss: 4.8142 - val_accuracy: 0.0520\n",
      "Epoch 30/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.2600 - accuracy: 0.3655 - val_loss: 5.0667 - val_accuracy: 0.0408\n",
      "Epoch 31/32\n",
      "2843/2843 [==============================] - 0s 93us/sample - loss: 2.1529 - accuracy: 0.4059 - val_loss: 5.3644 - val_accuracy: 0.0605\n",
      "Epoch 32/32\n",
      "2843/2843 [==============================] - 0s 115us/sample - loss: 2.0828 - accuracy: 0.4298 - val_loss: 5.3838 - val_accuracy: 0.0520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x293ede2bba8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, input_dim=25, activation = \"relu\"),\n",
    "    keras.layers.Dense(64, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "ae_dnn_clf = keras.models.Sequential([sparse_l1_encoder, dnn_clf])\n",
    "ae_dnn_clf.compile(loss = \"categorical_crossentropy\", optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "ae_dnn_clf.fit(X_std_train, y_std_train, batch_size = 32, epochs = 32,\n",
    "              validation_data = [X_std_val, y_std_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "889/1 [==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 35us/sample - loss: 4.4316 - accuracy: 0.0439\n"
     ]
    }
   ],
   "source": [
    "y_pred = ae_dnn_clf.predict(X_std_test)\n",
    "score = ae_dnn_clf.evaluate(X_std_test, y_std_test)\n",
    "\n",
    "df_testing_result = pd.DataFrame(index = range(len(y_pred)), columns = [\"Predict\", \"Actual\"])\n",
    "for i in range(len(y_pred)):\n",
    "    df_testing_result.loc[i, \"Predict\"] = np.argmax(y_pred[i])\n",
    "    df_testing_result.loc[i, \"Actual\"] = np.argmax(y_test[i])\n",
    "    \n",
    "df_testing_result.loc[:, \"Error between +1.0 ~ -1.0\"] = df_testing_result.apply(lambda x : \"True\" if abs(x[0] - x[1]) <=  else \"false\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.043869517743587494\n",
      "accuracy between +1.0 ~ -1.0 : 0.9471316085489314\n"
     ]
    }
   ],
   "source": [
    "score_interval = len(df_testing_result[df_testing_result['Error between +1.0 ~ -1.0'] == 'True']) / len(df_testing_result)\n",
    "\n",
    "print(f'accuracy : {score[1]}')\n",
    "print(f'accuracy between +1.0 ~ -1.0 : {score_interval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=35, activation = \"relu\"))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1220 17:05:19.291732  8268 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2843 samples, validate on 711 samples\n",
      "Epoch 1/32\n",
      "2843/2843 [==============================] - 1s 226us/sample - loss: 15.5282 - accuracy: 0.0338 - val_loss: 15.2807 - val_accuracy: 0.0492\n",
      "Epoch 2/32\n",
      "2843/2843 [==============================] - 0s 74us/sample - loss: 15.4666 - accuracy: 0.0366 - val_loss: 15.2463 - val_accuracy: 0.0478\n",
      "Epoch 3/32\n",
      "2843/2843 [==============================] - 0s 65us/sample - loss: 15.3936 - accuracy: 0.0422 - val_loss: 15.3034 - val_accuracy: 0.0464\n",
      "Epoch 4/32\n",
      "2843/2843 [==============================] - 0s 65us/sample - loss: 15.3895 - accuracy: 0.0426 - val_loss: 15.2249 - val_accuracy: 0.0520\n",
      "Epoch 5/32\n",
      "2843/2843 [==============================] - 0s 59us/sample - loss: 15.4579 - accuracy: 0.0390 - val_loss: 15.4152 - val_accuracy: 0.0422\n",
      "Epoch 6/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5408 - accuracy: 0.0341 - val_loss: 15.3618 - val_accuracy: 0.0450\n",
      "Epoch 7/32\n",
      "2843/2843 [==============================] - 0s 65us/sample - loss: 15.5263 - accuracy: 0.0352 - val_loss: 15.4306 - val_accuracy: 0.0408\n",
      "Epoch 8/32\n",
      "2843/2843 [==============================] - 0s 64us/sample - loss: 15.5304 - accuracy: 0.0355 - val_loss: 15.3275 - val_accuracy: 0.0464\n",
      "Epoch 9/32\n",
      "2843/2843 [==============================] - 0s 70us/sample - loss: 15.4939 - accuracy: 0.0380 - val_loss: 15.4117 - val_accuracy: 0.0422\n",
      "Epoch 10/32\n",
      "2843/2843 [==============================] - 0s 57us/sample - loss: 15.5627 - accuracy: 0.0331 - val_loss: 15.3607 - val_accuracy: 0.0450\n",
      "Epoch 11/32\n",
      "2843/2843 [==============================] - 0s 57us/sample - loss: 15.5417 - accuracy: 0.0348 - val_loss: 15.4089 - val_accuracy: 0.0422\n",
      "Epoch 12/32\n",
      "2843/2843 [==============================] - 0s 57us/sample - loss: 15.5885 - accuracy: 0.0317 - val_loss: 15.3667 - val_accuracy: 0.0450\n",
      "Epoch 13/32\n",
      "2843/2843 [==============================] - 0s 53us/sample - loss: 15.5622 - accuracy: 0.0341 - val_loss: 15.3618 - val_accuracy: 0.0450\n",
      "Epoch 14/32\n",
      "2843/2843 [==============================] - 0s 53us/sample - loss: 15.6007 - accuracy: 0.0320 - val_loss: 15.4072 - val_accuracy: 0.0422\n",
      "Epoch 15/32\n",
      "2843/2843 [==============================] - 0s 54us/sample - loss: 15.5930 - accuracy: 0.0324 - val_loss: 15.4086 - val_accuracy: 0.0422\n",
      "Epoch 16/32\n",
      "2843/2843 [==============================] - 0s 57us/sample - loss: 15.5746 - accuracy: 0.0338 - val_loss: 15.4559 - val_accuracy: 0.0394\n",
      "Epoch 17/32\n",
      "2843/2843 [==============================] - 0s 54us/sample - loss: 15.5869 - accuracy: 0.0327 - val_loss: 15.4111 - val_accuracy: 0.0422\n",
      "Epoch 18/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5856 - accuracy: 0.0331 - val_loss: 15.4118 - val_accuracy: 0.0422\n",
      "Epoch 19/32\n",
      "2843/2843 [==============================] - 0s 53us/sample - loss: 15.5799 - accuracy: 0.0334 - val_loss: 15.4352 - val_accuracy: 0.0408\n",
      "Epoch 20/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5799 - accuracy: 0.0334 - val_loss: 15.4353 - val_accuracy: 0.0408\n",
      "Epoch 21/32\n",
      "2843/2843 [==============================] - 0s 54us/sample - loss: 15.5798 - accuracy: 0.0334 - val_loss: 15.4357 - val_accuracy: 0.0408\n",
      "Epoch 22/32\n",
      "2843/2843 [==============================] - 0s 52us/sample - loss: 15.5798 - accuracy: 0.0334 - val_loss: 15.4359 - val_accuracy: 0.0408\n",
      "Epoch 23/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5741 - accuracy: 0.0338 - val_loss: 15.4360 - val_accuracy: 0.0408\n",
      "Epoch 24/32\n",
      "2843/2843 [==============================] - 0s 59us/sample - loss: 15.5797 - accuracy: 0.0334 - val_loss: 15.4363 - val_accuracy: 0.0408\n",
      "Epoch 25/32\n",
      "2843/2843 [==============================] - 0s 57us/sample - loss: 15.5684 - accuracy: 0.0341 - val_loss: 15.4364 - val_accuracy: 0.0408\n",
      "Epoch 26/32\n",
      "2843/2843 [==============================] - 0s 59us/sample - loss: 15.5684 - accuracy: 0.0341 - val_loss: 15.4368 - val_accuracy: 0.0408\n",
      "Epoch 27/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5683 - accuracy: 0.0341 - val_loss: 15.4143 - val_accuracy: 0.0422\n",
      "Epoch 28/32\n",
      "2843/2843 [==============================] - 0s 53us/sample - loss: 15.5627 - accuracy: 0.0345 - val_loss: 15.4597 - val_accuracy: 0.0394\n",
      "Epoch 29/32\n",
      "2843/2843 [==============================] - 0s 58us/sample - loss: 15.5683 - accuracy: 0.0341 - val_loss: 15.4373 - val_accuracy: 0.0408\n",
      "Epoch 30/32\n",
      "2843/2843 [==============================] - 0s 55us/sample - loss: 15.5683 - accuracy: 0.0341 - val_loss: 15.4600 - val_accuracy: 0.0394\n",
      "Epoch 31/32\n",
      "2843/2843 [==============================] - 0s 54us/sample - loss: 15.5626 - accuracy: 0.0345 - val_loss: 15.4602 - val_accuracy: 0.0394\n",
      "Epoch 32/32\n",
      "2843/2843 [==============================] - 0s 53us/sample - loss: 15.5513 - accuracy: 0.0352 - val_loss: 15.4604 - val_accuracy: 0.0394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20189ec9dd8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=32, batch_size=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model on Standarized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_std = Sequential()\n",
    "model_std.add(Dense(20, input_dim=35, activation = \"relu\"))\n",
    "model_std.add(Dense(64, activation='relu'))\n",
    "model_std.add(Dense(128, activation='relu'))\n",
    "model_std.add(Dense(64, activation='relu'))\n",
    "model_std.add(Dense(100, activation=\"softmax\"))\n",
    "model_std.compile(loss=\"categorical_crossentropy\", optimizer='Adam', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2843 samples, validate on 711 samples\n",
      "Epoch 1/32\n",
      "2843/2843 [==============================] - 2s 568us/sample - loss: 4.4436 - accuracy: 0.0229 - val_loss: 4.1542 - val_accuracy: 0.0267\n",
      "Epoch 2/32\n",
      "2843/2843 [==============================] - 0s 37us/sample - loss: 3.9457 - accuracy: 0.0376 - val_loss: 3.8537 - val_accuracy: 0.0338\n",
      "Epoch 3/32\n",
      "2843/2843 [==============================] - 0s 39us/sample - loss: 3.7816 - accuracy: 0.0521 - val_loss: 3.7987 - val_accuracy: 0.0422\n",
      "Epoch 4/32\n",
      "2843/2843 [==============================] - 0s 39us/sample - loss: 3.7143 - accuracy: 0.0492 - val_loss: 3.7561 - val_accuracy: 0.0422\n",
      "Epoch 5/32\n",
      "2843/2843 [==============================] - 0s 37us/sample - loss: 3.6572 - accuracy: 0.0594 - val_loss: 3.7295 - val_accuracy: 0.0323\n",
      "Epoch 6/32\n",
      "2843/2843 [==============================] - 0s 39us/sample - loss: 3.5929 - accuracy: 0.0711 - val_loss: 3.6867 - val_accuracy: 0.0394\n",
      "Epoch 7/32\n",
      "2843/2843 [==============================] - 0s 46us/sample - loss: 3.5267 - accuracy: 0.0718 - val_loss: 3.6468 - val_accuracy: 0.0478\n",
      "Epoch 8/32\n",
      "2843/2843 [==============================] - 0s 50us/sample - loss: 3.4744 - accuracy: 0.0858 - val_loss: 3.6134 - val_accuracy: 0.0492\n",
      "Epoch 9/32\n",
      "2843/2843 [==============================] - 0s 41us/sample - loss: 3.4226 - accuracy: 0.0813 - val_loss: 3.6026 - val_accuracy: 0.0534\n",
      "Epoch 10/32\n",
      "2843/2843 [==============================] - 0s 41us/sample - loss: 3.3878 - accuracy: 0.0872 - val_loss: 3.5810 - val_accuracy: 0.0506\n",
      "Epoch 11/32\n",
      "2843/2843 [==============================] - 0s 38us/sample - loss: 3.3550 - accuracy: 0.0907 - val_loss: 3.5777 - val_accuracy: 0.0450\n",
      "Epoch 12/32\n",
      "2843/2843 [==============================] - 0s 35us/sample - loss: 3.3171 - accuracy: 0.0985 - val_loss: 3.5673 - val_accuracy: 0.0577\n",
      "Epoch 13/32\n",
      "2843/2843 [==============================] - 0s 36us/sample - loss: 3.2818 - accuracy: 0.1101 - val_loss: 3.5800 - val_accuracy: 0.0506\n",
      "Epoch 14/32\n",
      "2843/2843 [==============================] - 0s 35us/sample - loss: 3.2539 - accuracy: 0.1041 - val_loss: 3.5690 - val_accuracy: 0.0549\n",
      "Epoch 15/32\n",
      "2843/2843 [==============================] - 0s 35us/sample - loss: 3.2239 - accuracy: 0.1129 - val_loss: 3.5956 - val_accuracy: 0.0549\n",
      "Epoch 16/32\n",
      "2843/2843 [==============================] - 0s 35us/sample - loss: 3.2042 - accuracy: 0.1136 - val_loss: 3.5914 - val_accuracy: 0.0619\n",
      "Epoch 17/32\n",
      "2843/2843 [==============================] - 0s 34us/sample - loss: 3.1684 - accuracy: 0.1235 - val_loss: 3.5830 - val_accuracy: 0.0591\n",
      "Epoch 18/32\n",
      "2843/2843 [==============================] - 0s 45us/sample - loss: 3.1542 - accuracy: 0.1185 - val_loss: 3.5914 - val_accuracy: 0.0717\n",
      "Epoch 19/32\n",
      "2843/2843 [==============================] - 0s 55us/sample - loss: 3.1285 - accuracy: 0.1235 - val_loss: 3.6047 - val_accuracy: 0.0520\n",
      "Epoch 20/32\n",
      "2843/2843 [==============================] - 0s 46us/sample - loss: 3.0940 - accuracy: 0.1375 - val_loss: 3.5963 - val_accuracy: 0.0689\n",
      "Epoch 21/32\n",
      "2843/2843 [==============================] - 0s 52us/sample - loss: 3.0770 - accuracy: 0.1498 - val_loss: 3.6177 - val_accuracy: 0.0675\n",
      "Epoch 22/32\n",
      "2843/2843 [==============================] - 0s 40us/sample - loss: 3.0550 - accuracy: 0.1407 - val_loss: 3.6480 - val_accuracy: 0.0619\n",
      "Epoch 23/32\n",
      "2843/2843 [==============================] - 0s 42us/sample - loss: 3.0262 - accuracy: 0.1579 - val_loss: 3.6535 - val_accuracy: 0.0563\n",
      "Epoch 24/32\n",
      "2843/2843 [==============================] - 0s 44us/sample - loss: 2.9988 - accuracy: 0.1534 - val_loss: 3.6676 - val_accuracy: 0.0534\n",
      "Epoch 25/32\n",
      "2843/2843 [==============================] - 0s 42us/sample - loss: 2.9788 - accuracy: 0.1562 - val_loss: 3.6843 - val_accuracy: 0.0619\n",
      "Epoch 26/32\n",
      "2843/2843 [==============================] - 0s 45us/sample - loss: 2.9631 - accuracy: 0.1622 - val_loss: 3.6722 - val_accuracy: 0.0633\n",
      "Epoch 27/32\n",
      "2843/2843 [==============================] - 0s 45us/sample - loss: 2.9321 - accuracy: 0.1699 - val_loss: 3.7180 - val_accuracy: 0.0605\n",
      "Epoch 28/32\n",
      "2843/2843 [==============================] - 0s 45us/sample - loss: 2.9060 - accuracy: 0.1790 - val_loss: 3.7288 - val_accuracy: 0.0563\n",
      "Epoch 29/32\n",
      "2843/2843 [==============================] - 0s 39us/sample - loss: 2.8909 - accuracy: 0.1759 - val_loss: 3.7661 - val_accuracy: 0.0703\n",
      "Epoch 30/32\n",
      "2843/2843 [==============================] - 0s 39us/sample - loss: 2.8617 - accuracy: 0.1913 - val_loss: 3.7821 - val_accuracy: 0.0619\n",
      "Epoch 31/32\n",
      "2843/2843 [==============================] - 0s 46us/sample - loss: 2.8393 - accuracy: 0.1949 - val_loss: 3.8254 - val_accuracy: 0.0675\n",
      "Epoch 32/32\n",
      "2843/2843 [==============================] - 0s 45us/sample - loss: 2.8194 - accuracy: 0.2054 - val_loss: 3.8003 - val_accuracy: 0.0619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201b223c048>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_std.fit(X_std_train, y_std_train, epochs=32, batch_size=100, validation_data=(X_std_val, y_std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_std = model_std.predict(X_std_test)\n",
    "score_std = model_std.evaluate(X_std_test, y_std_test)\n",
    "\n",
    "df_result_std = pd.DataFrame(index = range(len(y_pred)), columns = [\"Predict\", \"Actual\"])\n",
    "for i in range(len(y_pred)):\n",
    "    df_result_std.loc[i, \"Predict\"] = np.argmax(y_pred_std[i])\n",
    "    df_result_std.loc[i, \"Actual\"] = np.argmax(y_test[i])\n",
    "    \n",
    "df_result_std.loc[:, \"Error between +1.0 ~ -1.0\"] = df_result_std.apply(lambda x : \"True\" if abs(x[0] - x[1]) <= 10 else \"false\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_interval_std = len(df_testing_result[df_testing_result['Error between +1.0 ~ -1.0'] == 'True']) / len(df_testing_result)\n",
    "\n",
    "print(f'accuracy : {score_std[1]}')\n",
    "print(f'accuracy between +1.0 ~ -1.0 : {score_interval_std}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
